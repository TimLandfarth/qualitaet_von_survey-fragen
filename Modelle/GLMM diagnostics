General diagnosics of GLMM´s

This is a short overview of Model assumptions of GLMM`S and problems of model diagnostics of that model class and how we could tackle that issue with the DHARMa package.

Source:  https://www.theanalysisfactor.com/regression-diagnostics-glmm/

How to check if a GLMM gives a good fit? Unfortunately not a straightforward answer
Model-Assumptions:
1.	First Assumption: random effects come from a normal distribution
 visual methods of checking normality (histograms, Q-Q-Plots of each of the random effects) 
	Problem: when this assumption is hurt, there are no easy remedies
	 in linear models the outcome can be transformed, in GLMM´s they cannot
2.	Second assumption: The chosen link function is appropriate
Most typical link functions as logit or poisson don´t need to be good representations of the relationship of the predictors with the outcomes.
Problem: Can become quite complicated the bigger the models become
First way: compare fitted and actual outcomes. (not perfect)
Second way: with most GLMM´s its best to compare averages of outcomes to predicted values.  if general form of the model is correct, differences between predicted values and averaged actual values will be small. Important: no patterns should be visible in these differences. This is similar to the idea of Hosmer-Lemeshow for logistic regression models
Use Pearson (normale) residuals or deviance residuals.
Link zur Erklärung von allen möglichen Residuen:

https://r-forge.r-project.org/scm/viewvc.php/*checkout*/pkg/BinomTools/inst/ResidualsGLM.pdf?revision=6&root=binomtools&pathrev=6

3.	Third assumption: Appropriate estimation of variance
Checking the variability of the outcomes. Not as easy as in linear models since the variance is a function of the parameter being estimated.
This is fortunately implemented in R. with the chi-squared statistic. (Ich glaube das ist der Dispersionsparameter).
Diagnostics for overdispersed models( bigger then 1) vary across distributions, but generally there are remedies for that which result in more conservative p-values.
4.	Conditional on the random effects, the response values Y_ij are independent of each other and follow a distribution from the exponential family.


Model-diagnostics with DHARMa (R-Package)

Source: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html

1.)	Problems of GL(M)M´s and how DHARMa tries to solve these problems

Problem in GL(M)M´s:  Interpretation of conventional residuals (Deviance,Pearson and raw) is problematic. One reason for that is that the expected distribution of the data changes with the fitted values. Reweighing with the expected distribution, as done in Pearson residuals, or using deviance residuals, helps to some extent, but it does not lead to homogenous residuals, even if the model is correctly specified.  unreliable interpretation of residual plots.

Current standard practice: eyeball residual plots for major misspecifications, and potentially have a look at the random effect distribution, then make a test for overdispersion.
 usually positive, so adjust model for overdispersion.

Drawbacks: 
1.) overdispersion is often result of missing predictors and misspecified model  residual plots make it difficult to identify those issues.
2.) Dispersion varies with predictors  can have significant effect on inference. Heteroscedasticity hardly ever tested in GLMM´s eventhough it is assumed to be as likely and influential as in linear regression.
3.) Residuals are checked conditional on the random effects  no check on the entire models structure.

How does DHARMa tackle these issues?

DHARMa creates readily interpretable residuals for GLMM`s that are standardized between 0 and 1 that can be interpreted as intuitively as residuals for the linear model. This is done by a simulation-based approach. The steps are:
1.)	Simulate new response data from the fitted model for each observation
2.)	For each observation, calculate the empirical cumulative density function for the simulated observations, which describe the possible values at the predictor combination of the observed value, assuming the fitted model is correct.
3.)	The residual is then defined as the value of the empirical density function at the value of the observed data, so a residual of 0 means that all simulated values ar larger than the observed value, and a residual of 0,5 means half of the simulated values are larger than the observed value.

What is the advantage of this definition of residuals?

The key advantage is that these residuals always have the same, known distribution, independent of the model that is fit, if the model is correctly specified. To see this, note that, if the observed data was created from the same data-generating process that we simulate from, all values of the cumulative distribution should appear with equal probability.  we expect the distribution of the residuals to be flat.

Workflow in DHARMa

1.)	Getting standardized residuals according to the above-mentioned algorithm:

     Stand_resid <- simulateResiduals(fittedModel = fittedModel, plot = F)
	
FittedModel needs to be fitted model that is supported by DHARMa (glmer or glmmTMB for example)
2.)	Testing for Dispersion:
DHARMa contains three overdispersion tests that compare the dispersion of simulated residuals to the observed residuals

2.1) (Default) Non-parametric: comparison of the variance of simulated and observed residuals
2.2) Pearson – chi2 test
2.3) refit: if residuals are done via refit  DHARMa compared refitted and original Pearson residuals

INFO: DHARMa default Option 1 is considered b

3.)	Testing for Zero-Inflation

Funktion: testZeroInflation(Stand_Resi)
Compares the distribution of expected zeros in the data against observed zeros

4.)	Testing for Heteroscedasticity
5.)	Residual correlation structure

Functions:	 testTemporalAutocorrelation based on Durbin-Watson test
		 testSpat
